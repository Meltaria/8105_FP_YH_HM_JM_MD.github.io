---
title: "Linear Regression"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
---

```{r preparation, include=FALSE}
library(tidyverse)
library(glmnet)
library(GGally)
library(patchwork)
library(gt)
library(leaps)
library(caret)
library(modelr)
library(nnet)
library(ggridges)
load("merged_nhanes.RData")
set.seed(1)
```

## Exploratory Analysis ##

```{r}
summary(nhanes)
```

```{r}
pair_plots = nhanes %>%
  relocate(hepbvax) %>%
  ggpairs()
pair_plots
```

According to the plots, we may found most of the variable distributions are normal, it would not be necessary to make some transformation. 

As we have three age variables in the data set and for each analysis, we may need only one of them. Thus we may separate the data set into dataset with age_decile, age_quartile and age_crude.

```{r}
nhanes_decile = nhanes %>% 
  select( -age_quartile, -agecrude)

nhanes_quartile = nhanes %>%
  select( -age_decile, -agecrude)

nhanes_crude = nhanes %>%
  select( -age_decile, -age_quartile)
```

Here, let's build two logistic regression models based on the NHANES data set with different age variables.

```{r}
fit1 = glm(hepbvax ~ ., data = nhanes_decile, family = binomial())
summary(fit1)

fit2 = glm(hepbvax ~ ., data = nhanes_quartile, family = binomial())
summary(fit2)

fit3 = glm(hepbvax ~ ., data = nhanes_crude, family = binomial())
summary(fit3)
```

## Automatic Selection ##

Here we would like to do automatic selection to choose the best subsets for our models.

### Model with age_decile ###

__Backward Selection__
```{r}
step(fit1, direction = "backward")
```

The model we obtained is __glm(formula = hepbvax ~ age_decile + gender + race + educat + insany, family = binomial(), data = nhanes_decile)__ 

__Forward Selection__
```{r}
step(fit1, direction = "forward")
```

The model we obtained is __glm(formula = hepbvax ~ age_decile + gender + race + poverty + educat + insany + hepb + hepc, family = binomial(), data = nhanes_decile)__

### Model with age_quartile ###

__Backward Selection__
```{r}
step(fit2, direction = "backward")
```

The model we obtained is __glm(formula = hepbvax ~ age_quartile + gender + race + educat + insany, family = binomial(), data = nhanes_quartile)__ 

__Forward Selection__
```{r}
step(fit2, direction = "forward")
```

The model we obtained is __glm(formula = hepbvax ~ age_quartile + gender + race + poverty + educat + insany + hepb + hepc, family = binomial(), data = nhanes_quartile)__

### Model with age_crude ###
__Backward Selection__
```{r}
step(fit3, direction = "backward")
```

The model we obtained is __glm(formula = hepbvax ~ agecrude + gender + race + educat + insany, family = binomial(), data = nhanes_crude)__ 

__Forward Selection__
```{r}
step(fit3, direction = "forward")
```

The model we obtained is __glm(formula = hepbvax ~ agecrude + gender + race + poverty + educat + insany + hepb + hepc, family = binomial(), data = nhanes_crude)__

Comparing the AIC of the model with different variables, we can find that the models with age_crude has the smallest AIC. Thus we will choose age_crude as our age variable in the model.

The model obtained from Backward elimination and forward selection were different. The Forward model three extra predictors included than that of backward: poverty, hepb and hepc. To choose a better model, further steps may be required. 

## Model Comparing ##
Let's fit the models from backward elimination and forward selection.

```{r}
fit_small = glm(formula = hepbvax ~ agecrude + gender + race + educat + insany, family = binomial(), data = nhanes_crude)
fit_large = glm(formula = hepbvax ~ agecrude + gender + race + poverty + educat + insany + hepb + hepc, family = binomial(), data = nhanes_crude)
```

Anova would be used to make selection between the small model and the large model.

```{r}
anova(fit_small, fit_large, test = "LR")
```

As the p value is 0.3299, larger than 0.05. Thus we can conclude that the larger model is not superior. The model we would use would be the smaller model with age_crude variable.

The model: __fit_small = glm(formula = hepbvax ~ agecrude + gender + race + educat + insany, family = binomial(), data = nhanes_crude)__

## Tidy out ##
```{r}
model = fit_small

model %>% 
  broom::glance()

model %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

```{r}
nhanes_pred = 
  nhanes_crude %>% 
  modelr::add_predictions(model) %>% 
  mutate(sample = as.numeric(hepbvax)) %>%
  pivot_longer(pred:sample,
               names_to = "name",
               values_to = "value")

nhanes_pred %>%
  ggplot(aes(x = value, color = name)) + 
  geom_histogram(position = "dodge") + 
  labs(title = "Hepbvax_prediction")
```

From this graph, we can see that the linear regression has poor prediction power for our model. Therefore, we will introduce multinomial logistic regression, and we will use the same predictors as we do in our linear model.

```{r}
multi_nomial_reg = multinom(hepbvax ~ agecrude + gender + race + educat + insany, data = nhanes_crude)
summary(multi_nomial_reg)
```

The model we obtained is __multinom(formula = hepbvax ~ agecrude + gender + race + educat + insany,  data = nhanes_crude)__

```{r}
multi_pred = 
  nhanes_crude %>% 
  modelr::add_predictions(multi_nomial_reg) %>% 
  mutate(True_OR_False = ifelse(pred == hepbvax, TRUE, FALSE))

multi_pred %>% 
  ggplot(aes(x = agecrude)) + 
  geom_histogram() + 
  facet_grid(. ~ True_OR_False) + 
  labs(title = "True_OR_False v.s age_crude")
```

We make a plot to show the prediction accuracy of or multinomial logistic model with respect to age_crude. This graph brings us to wonder: is there a tendency of predict accuracy with respect to age_crude?

```{r}
cal_1 =  
  multi_pred %>% 
  group_by(agecrude, True_OR_False) %>% 
  summarize(count = n()) %>% 
  filter(True_OR_False == TRUE) %>% 
  select(-True_OR_False)

cal_2 = multi_pred %>% 
  group_by(agecrude) %>% 
  summarize(count = n())

cal = left_join(cal_1,cal_2, by = "agecrude") %>% 
  mutate(accuracy = count.x/count.y)

cal %>% 
  ggplot(aes(x = agecrude, y = accuracy)) +
  geom_smooth() + 
  labs(title = "Predict accuracy v.s age_crude")
```

From this graph, we can see that, overall, our model has a predict accuracy higher than 0.6, and for people older than 40, the accuracy rises as the age increases.
