---
title: "Linear Regression"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
---

```{r preparation, include=FALSE}
library(tidyverse)
library(glmnet)
library(GGally)
library(patchwork)
library(gt)
library(leaps)
library(caret)
load("merged_nhanes.RData")
set.seed(1)
```

### Exploratory Analysis ###

```{r}
summary(nhanes)
```

```{r}
pair_plots = nhanes %>%
  relocate(hepbvax) %>%
  ggpairs()
pair_plots
```

According to the plots, we may found most of the variable distributions are normal, it would not be necessary to make some transformation. 

As we have three age variables in the data set and for each analysis, we may need only one of them. Thus we may separate the data set into dataset with age_decile, age_quartile and age_crude. 
```{r}
nhanes_decile = nhanes %>% 
  select( -age_quartile, -agecrude)

nhanes_quartile = nhanes %>%
  select( -age_decile, -agecrude)

nhanes_crude = nhanes %>%
  select( -age_decile, -age_quartile)
```

Here, let's build two logistic regression model based on nhanes data set with different age_variables. 
```{r}
fit1 = glm(hepbvax ~ ., data = nhanes_decile, family = binomial())
summary(fit1)

fit2 = glm(hepbvax ~ ., data = nhanes_quartile, family = binomial())
summary(fit2)

fit3 = glm(hepbvax ~ ., data = nhanes_crude, family = binomial())
summary(fit3)
```

### Automatic selection
Here we would like to do automatic selection to choose the best subsets for our models. 
#### Model with age_decile
__Backward Selection__
```{r}
step(fit1, direction = "backward")
```
The model we obtained is __glm(formula = hepbvax ~ age_decile + gender + race + educat + insany, family = binomial(), data = nhanes_decile)__ 

__Forward Selection__
```{r}
step(fit1, direction = "forward")
```
The model we obtained is __glm(formula = hepbvax ~ age_decile + gender + race + poverty + educat + insany + hepb + hepc, family = binomial(), data = nhanes_decile)__

#### Model with age_quartile
__Backward Selection__
```{r}
step(fit2, direction = "backward")
```
The model we obtained is __glm(formula = hepbvax ~ age_quartile + gender + race + educat + insany, family = binomial(), data = nhanes_quartile)__ 

__Forward Selection__
```{r}
step(fit2, direction = "forward")
```
The model we obtained is __glm(formula = hepbvax ~ age_quartile + gender + race + poverty + educat + insany + hepb + hepc, family = binomial(), data = nhanes_quartile)__

#### Model with age_crude
__Backward Selection__
```{r}
step(fit3, direction = "backward")
```
The model we obtained is __glm(formula = hepbvax ~ agecrude + gender + race + educat + insany, family = binomial(), data = nhanes_crude)__ 

__Forward Selection__
```{r}
step(fit3, direction = "forward")
```
The model we obtained is __glm(formula = hepbvax ~ agecrude + gender + race + poverty + educat + insany + hepb + hepc, family = binomial(), data = nhanes_crude)__


Comparing the AIC of the model with different variables, we can find that the models with age_crude has the smallest AIC. Thus we will choose age_crude as our age variable in the model.

The model obtained from Backward elimination and forward selection were different. The Forward model three extra predictors included than that of backward: poverty, hepb and hepc. To choose a better model, further steps may be required. 

### Model comparing
Let's fit the models from backward elimination and forward selection.
```{r}
fit_small = glm(formula = hepbvax ~ agecrude + gender + race + educat + insany, family = binomial(), data = nhanes_crude)
fit_large = glm(formula = hepbvax ~ agecrude + gender + race + poverty + educat + insany + hepb + hepc, family = binomial(), data = nhanes_crude)
```
Anova would be used to make selection between the small model and the large model. 
```{r}
anova(fit_small, fit_large, test = "LR")
```
As the p value is 0.3299, larger than 0.05. Thus we can conclude that the larger model is not superior. The model we would use would be the smaller model with agecrude variable.
The model: __fit_small = glm(formula = hepbvax ~ agecrude + gender + race + educat + insany, family = binomial(), data = nhanes_crude)__

### Tidy out 
```{r}
model = fit_small

model %>% 
  broom::glance()

model %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```


